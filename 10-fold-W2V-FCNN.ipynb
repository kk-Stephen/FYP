{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b079bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/ROG/OneDrive/桌面/FYP/Dataset/Train_data/train_data_after_washing.csv'\n",
    "test_dir = 'C:/Users/ROG/OneDrive/桌面/FYP/Dataset/Test_data/test_data_after_washing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65681c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import multiprocessing\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score, cohen_kappa_score, roc_curve, auc, make_scorer, accuracy_score, f1_score \n",
    "tf.get_logger().setLevel('ERROR') # return ERROR messages, ignore others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12cce2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot\n",
    "def encode_one_hot(ori_dataframe):\n",
    "    dummies = pd.get_dummies(ori_dataframe)\n",
    "    res = pd.concat([ori_dataframe, dummies], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b901eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv(train_dir)\n",
    "data = pd.DataFrame(data)\n",
    "test_data = pd.read_csv(test_dir)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "train_data = pd.DataFrame()\n",
    "val_data = pd.DataFrame()\n",
    "data = data[[\"review\",\"rating\"]]\n",
    "test_data = test_data[[\"review\",\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a75f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = data['rating'].map({1 : 0,\n",
    "                                     2 : 0,\n",
    "                                     3 : 0,\n",
    "                                     4 : 0,\n",
    "                                     5 : 1,\n",
    "                                     6 : 1,\n",
    "                                     7 : 1,\n",
    "                                     8 : 1,\n",
    "                                     9 : 2,\n",
    "                                     10 : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79591ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['labels'] = test_data['rating'].map({1 : 0,\n",
    "                                               2 : 0,\n",
    "                                               3 : 0,\n",
    "                                               4 : 0,\n",
    "                                               5 : 1,\n",
    "                                               6 : 1,\n",
    "                                               7 : 1,\n",
    "                                               8 : 1,\n",
    "                                               9 : 2,\n",
    "                                               10 : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5666bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data\n",
    "data = data[[\"review\",\"labels\"]]\n",
    "test_data = test_data[[\"review\",\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc8c4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english') #remove useless words\n",
    "    tokens = nltk.word_tokenize(text) #Tokenizers divide strings into lists of substrings\n",
    "    lower = [word.lower() for word in tokens] #remove uppercase\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    clean_text = lemm_text\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8c8a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize(data,tfidf_vect_fit):\n",
    "    X_tfidf = tfidf_vect_fit.transform(data) #Transform doc to matrix \n",
    "    words = tfidf_vect_fit.get_feature_names_out() #Get features names\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "    X_tfidf_df.columns = words\n",
    "    return(X_tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03ff592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(s):\n",
    "    inputs = tf.keras.Input(shape=(s,))\n",
    "    net = tf.keras.layers.Dense(128, activation='gelu', name='classifier_dense_1')(inputs)\n",
    "    net = tf.keras.layers.Dropout(0.3)(net)\n",
    "    net = tf.keras.layers.Dense(128, activation='gelu', name='classifier_dense_2')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(128, activation='gelu', name='classifier_dense_3')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(128, activation='gelu', name='classifier_dense_4')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(3, activation='softmax', name='classifier_dense_5')(net)\n",
    "    return tf.keras.Model(inputs, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d7f79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff_loss(y_true,y_pred):\n",
    "    alpha = tf.constant([[1],[2],[1]], dtype=tf.float32) #adjust weight for each label\n",
    "    gamma = 1.25\n",
    "    epsilon = 1.e-7\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "    ce = -tf.math.log(y_t)\n",
    "    weight = tf.pow(tf.subtract(1., y_t), gamma)\n",
    "    fl = tf.matmul(tf.multiply(weight, ce), alpha)\n",
    "    loss = tf.reduce_mean(fl)\n",
    "    loss = tf.abs(loss-0.10) + 0.10\n",
    "    loss = tf.convert_to_tensor(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81df46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graded_precision(y_true, y_pred, weights):\n",
    "    precision_0 = precision_score(y_true, y_pred, labels=[0], average='macro')\n",
    "    precision_1 = precision_score(y_true, y_pred, labels=[1], average='macro')\n",
    "    precision_2 = precision_score(y_true, y_pred, labels=[2], average='macro')\n",
    "    gp = ( weights[0] * precision_0 + weights[1] * precision_1 + weights[2] * precision_2 ) / ( weights[0] + weights[1] + weights[2] )\n",
    "    return gp\n",
    "def graded_recall(y_true, y_pred, weights):\n",
    "    recall_0 = recall_score(y_true, y_pred, labels=[0], average='macro')\n",
    "    recall_1 = recall_score(y_true, y_pred, labels=[1], average='macro')\n",
    "    recall_2 = recall_score(y_true, y_pred, labels=[2], average='macro')\n",
    "    gr = ( weights[0] * recall_0 + weights[1] * recall_1 + weights[2] * recall_2 ) / ( weights[0] + weights[1] + weights[2] )\n",
    "    return gr\n",
    "def graded_f1(precision, recall):\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce710339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def model_training(x_train, y_train, x_val, y_val, times):\n",
    "    times = str(times)\n",
    "    t0 = time.time()\n",
    "    tfidf_vect = TfidfVectorizer(analyzer=clean)\n",
    "    tfidf_vect_fit=tfidf_vect.fit(x_train)\n",
    "    x_train = vectorize(x_train,tfidf_vect_fit)\n",
    "    x_val = vectorize(x_val,tfidf_vect_fit)\n",
    "    x_train = tf.convert_to_tensor(x_train.to_numpy())\n",
    "    y_train = tf.convert_to_tensor(y_train.to_numpy())\n",
    "    x_val = tf.convert_to_tensor(x_val.to_numpy())\n",
    "    y_val = tf.convert_to_tensor(y_val.to_numpy())\n",
    "    s = int(tf.shape(x_train)[1])   \n",
    "    classifier_model = build_classifier_model(s)\n",
    "    epochs = 5\n",
    "    steps_per_epoch = 0\n",
    "    for _ in x_train:\n",
    "        steps_per_epoch = steps_per_epoch + 1\n",
    "    num_train_steps = steps_per_epoch/24 * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "    init_lr = 3e-6\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=num_train_steps,\n",
    "                                              num_warmup_steps=num_warmup_steps,\n",
    "                                              optimizer_type='adamw')\n",
    "    classifier_model.compile(optimizer=optimizer,\n",
    "                             loss=ff_loss,\n",
    "                             metrics=[\"accuracy\", tf.keras.metrics.Recall(name='recall')])\n",
    "    file_name='WF_' + times\n",
    "    checkpoint_path = 'C:/Users/ROG/OneDrive/桌面/FYP/Model/'+ file_name + '/ckpt/cp.ckpt'\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "    csv_callback = tf.keras.callbacks.CSVLogger(\n",
    "    'C:/Users/ROG/OneDrive/桌面/FYP/Model/'+ file_name + '/record.csv', separator=',', append=False\n",
    "    )\n",
    "    print(times)\n",
    "    history = classifier_model.fit(x=x_train,\n",
    "                                   y=y_train,\n",
    "                                   validation_data=(x_val,y_val),\n",
    "                                   epochs=5,\n",
    "                                   batch_size=24,\n",
    "                                   callbacks=[cp_callback,csv_callback])\n",
    "    t1 = time.time()\n",
    "    time_train = t1-t0\n",
    "    del x_train\n",
    "    del x_val\n",
    "    del y_train\n",
    "    del y_val\n",
    "    return (tfidf_vect_fit,classifier_model,time_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0df893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing(encoder, model, X, y, times,val):\n",
    "    times = str(times)\n",
    "    #generate y_true and prediction results\n",
    "    y = tf.convert_to_tensor(y.to_numpy())\n",
    "    y_true = y\n",
    "    if val == True:\n",
    "        y_true = np.argmax(y, axis=1)    \n",
    "    X = vectorize(X, encoder)\n",
    "    X = tf.convert_to_tensor(X.to_numpy())\n",
    "    pred = np.argmax(model.predict(X),axis=1)\n",
    "    print(pred)\n",
    "    print(y_true)\n",
    "    #different metrics\n",
    "    acc = accuracy_score(y_true, pred)\n",
    "    weights = [2, 1, 1]\n",
    "    prec = graded_precision(y_true, pred, weights)\n",
    "    rec = graded_recall(y_true, pred, weights)\n",
    "    f1 = graded_f1(prec, rec)\n",
    "    kappa = cohen_kappa_score(y_true, pred)\n",
    "    print(acc, prec, rec, f1, kappa)\n",
    "    #CM\n",
    "    if val == False:\n",
    "        con_mat = confusion_matrix(y, pred)\n",
    "        con_mat_norm = con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis]     # 归一化\n",
    "        con_mat_norm = np.around(con_mat_norm, decimals=2)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(con_mat_norm, annot=True, cmap='Blues')\n",
    "        plt.ylim(0, 3)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        #save CM\n",
    "        file_name='WF_' + times\n",
    "        plt.savefig(fname='C:/Users/ROG/OneDrive/桌面/FYP/Model/'+ file_name + '/CM.png', dpi=300)\n",
    "        plt.close()\n",
    "    del y\n",
    "    del y_true\n",
    "    del X\n",
    "    del pred\n",
    "    return (acc, prec, rec, f1, kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdb93b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"review\",\"labels\"]]\n",
    "y = data[\"labels\"].to_numpy()\n",
    "Y = data[\"labels\"]\n",
    "test_data = test_data[[\"review\",\"labels\"]]\n",
    "X = data[\"review\"].to_numpy()\n",
    "x = data[\"review\"]\n",
    "test_X = test_data[\"review\"]\n",
    "test_y = test_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f58a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/5\n",
      "1166/1179 [============================>.] - ETA: 0s - loss: 1.1237 - accuracy: 0.4737 - recall: 0.0000e+00\n",
      "Epoch 1: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_0/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 7s 4ms/step - loss: 1.1235 - accuracy: 0.4743 - recall: 0.0000e+00 - val_loss: 1.1192 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1174/1179 [============================>.] - ETA: 0s - loss: 1.1145 - accuracy: 0.5122 - recall: 0.0000e+00\n",
      "Epoch 2: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_0/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 5s 4ms/step - loss: 1.1146 - accuracy: 0.5122 - recall: 0.0000e+00 - val_loss: 1.1099 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "Epoch 3/5\n",
      "1167/1179 [============================>.] - ETA: 0s - loss: 1.1055 - accuracy: 0.5121 - recall: 0.0000e+00\n",
      "Epoch 3: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_0/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 5s 4ms/step - loss: 1.1056 - accuracy: 0.5122 - recall: 0.0000e+00 - val_loss: 1.1018 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "Epoch 4/5\n",
      "1169/1179 [============================>.] - ETA: 0s - loss: 1.0989 - accuracy: 0.5122 - recall: 0.0000e+00\n",
      "Epoch 4: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_0/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 5s 4ms/step - loss: 1.0988 - accuracy: 0.5122 - recall: 0.0000e+00 - val_loss: 1.0966 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "Epoch 5/5\n",
      "1166/1179 [============================>.] - ETA: 0s - loss: 1.0952 - accuracy: 0.5125 - recall: 0.0000e+00\n",
      "Epoch 5: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_0/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 5s 4ms/step - loss: 1.0953 - accuracy: 0.5122 - recall: 0.0000e+00 - val_loss: 1.0950 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "99/99 [==============================] - 0s 3ms/step\n",
      "[2 2 2 ... 2 2 2]\n",
      "[2 2 2 ... 1 1 1]\n",
      "0.5124124761298536 0.1281031190324634 0.25 0.16940235690235692 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327/327 [==============================] - 1s 1ms/step\n",
      "[2 2 2 ... 2 2 2]\n",
      "tf.Tensor([2 2 0 ... 2 2 2], shape=(10434,), dtype=int64)\n",
      "0.5115967030860648 0.1278991757715162 0.25 0.1692239411615521 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch 1/5\n",
      "1177/1179 [============================>.] - ETA: 0s - loss: 1.1230 - accuracy: 0.4790 - recall: 0.0000e+00\n",
      "Epoch 1: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_1/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 6s 4ms/step - loss: 1.1230 - accuracy: 0.4791 - recall: 0.0000e+00 - val_loss: 1.1182 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1168/1179 [============================>.] - ETA: 0s - loss: 1.1131 - accuracy: 0.5122 - recall: 0.0000e+00\n",
      "Epoch 2: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_1/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 5s 4ms/step - loss: 1.1131 - accuracy: 0.5122 - recall: 0.0000e+00 - val_loss: 1.1079 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "Epoch 3/5\n",
      "1169/1179 [============================>.] - ETA: 0s - loss: 1.1035 - accuracy: 0.5121 - recall: 0.0000e+00\n",
      "Epoch 3: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_1/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 5s 4ms/step - loss: 1.1034 - accuracy: 0.5122 - recall: 0.0000e+00 - val_loss: 1.0992 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "Epoch 4/5\n",
      "1179/1179 [==============================] - ETA: 0s - loss: 1.0964 - accuracy: 0.5122 - recall: 0.0000e+00\n",
      "Epoch 4: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_1/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 5s 4ms/step - loss: 1.0964 - accuracy: 0.5122 - recall: 0.0000e+00 - val_loss: 1.0940 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n",
      "Epoch 5/5\n",
      "1173/1179 [============================>.] - ETA: 0s - loss: 1.0926 - accuracy: 0.5127 - recall: 0.0000e+00\n",
      "Epoch 5: saving model to C:/Users/ROG/OneDrive/桌面/FYP/Model/WF_1/ckpt\\cp.ckpt\n",
      "1179/1179 [==============================] - 5s 4ms/step - loss: 1.0928 - accuracy: 0.5122 - recall: 0.0000e+00 - val_loss: 1.0923 - val_accuracy: 0.5124 - val_recall: 0.0000e+00\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m train_time\u001b[38;5;241m.\u001b[39mappend(time_train)\n\u001b[0;32m     28\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m acc, prec, rec, f1, kappa \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m val_acc\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[0;32m     31\u001b[0m val_gp\u001b[38;5;241m.\u001b[39mappend(prec)\n",
      "Cell \u001b[1;32mIn [14], line 9\u001b[0m, in \u001b[0;36mmodel_testing\u001b[1;34m(encoder, model, X, y, times, val)\u001b[0m\n\u001b[0;32m      7\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)    \n\u001b[0;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorize(X, encoder)\n\u001b[1;32m----> 9\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(X),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n",
      "File \u001b[1;32mD:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "#10-fold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "val_acc = []\n",
    "val_gp = []\n",
    "val_gr = []\n",
    "val_f1 = []\n",
    "val_kp = []\n",
    "tes_acc = []\n",
    "tes_gp = []\n",
    "tes_gr = []\n",
    "tes_f1 = []\n",
    "tes_kp = []\n",
    "train_time = []\n",
    "times = 0\n",
    "for train_index, val_index in skf.split(X, y):\n",
    "    #physical_device = tf.config.list_physical_devices('GPU')[0] # get the first GPU device\n",
    "    #tf.config.experimental.set_memory_growth(physical_device, True)\n",
    "    X_train, X_val = x[train_index], x[val_index]\n",
    "    y_train, y_val = Y[train_index], Y[val_index]\n",
    "    y_train = encode_one_hot(y_train)\n",
    "    y_val = encode_one_hot(y_val)\n",
    "    y_train = y_train[[0,1,2]]\n",
    "    y_val = y_val[[0,1,2]]\n",
    "    encoder, model,time_train = model_training(X_train,y_train, X_val, y_val, times)\n",
    "    train_time.append(time_train)\n",
    "    \n",
    "    val = True\n",
    "    acc, prec, rec, f1, kappa = model_testing(encoder,model, X_val, y_val, times, val)\n",
    "    val_acc.append(acc)\n",
    "    val_gp.append(prec)\n",
    "    val_gr.append(rec)\n",
    "    val_f1.append(f1)\n",
    "    val_kp.append(kappa)\n",
    "    val = False\n",
    "    acc, prec, rec, f1, kappa = model_testing(encoder,model, test_X, test_y, times, val)\n",
    "    tes_acc.append(acc)\n",
    "    tes_gp.append(prec)\n",
    "    tes_gr.append(rec)\n",
    "    tes_f1.append(f1)\n",
    "    tes_kp.append(kappa)\n",
    "    tf.keras.backend.clear_session\n",
    "    del encoder\n",
    "    del model\n",
    "    times = times + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a8ccce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 1ms/step\n",
      "[2 2 2 ... 2 2 2]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.5124124761298536 0.1281031190324634 0.25 0.16940235690235692 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327/327 [==============================] - 0s 1ms/step\n",
      "[2 2 2 ... 2 2 2]\n",
      "tf.Tensor([2 2 0 ... 2 2 2], shape=(10434,), dtype=int64)\n",
      "0.5115967030860648 0.1278991757715162 0.25 0.1692239411615521 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\dev tools\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "    val = True\n",
    "    acc, prec, rec, f1, kappa = model_testing(encoder,model, X_val, y_val, times, val)\n",
    "    val_acc.append(acc)\n",
    "    val_gp.append(prec)\n",
    "    val_gr.append(rec)\n",
    "    val_f1.append(f1)\n",
    "    val_kp.append(kappa)\n",
    "    val = False\n",
    "    acc, prec, rec, f1, kappa = model_testing(encoder,model, test_X, test_y, times, val)\n",
    "    tes_acc.append(acc)\n",
    "    tes_gp.append(prec)\n",
    "    tes_gr.append(rec)\n",
    "    tes_f1.append(f1)\n",
    "    tes_kp.append(kappa)\n",
    "    tf.keras.backend.clear_session\n",
    "    del encoder\n",
    "    del model\n",
    "    times = times + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47befc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e0d0567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79.52303552627563, 78.848872423172]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c21841c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      2\u001b[0m tfidf_vect \u001b[38;5;241m=\u001b[39m TfidfVectorizer(analyzer\u001b[38;5;241m=\u001b[39mclean)\n\u001b[1;32m----> 3\u001b[0m tfidf_vect_fit\u001b[38;5;241m=\u001b[39mtfidf_vect\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m)\n\u001b[0;32m      4\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      5\u001b[0m time_train \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "    t0 = time.time()\n",
    "    tfidf_vect = TfidfVectorizer(analyzer=clean)\n",
    "    tfidf_vect_fit=tfidf_vect.fit(x_train)\n",
    "    t1 = time.time()\n",
    "    time_train = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55609b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrics_list = [val_acc, val_gp, val_gr, val_f1, val_kp, tes_acc, tes_gp, tes_gr, tes_f1, tes_kp, train_time]\n",
    "avg_results = []\n",
    "for matric in matrics_list:\n",
    "    total = 0\n",
    "    for item in matric:\n",
    "        total = total + item\n",
    "    avg_results.append(total/len(matric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee37b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/ROG/OneDrive/桌面/FYP/Model/W2V_FCNN.txt\", \"w\") as f:\n",
    "    f.write(\"val_acc: \")\n",
    "    for item in val_acc:\n",
    "        f.write(str(item))\n",
    "        if val_acc.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"val_gp: \")\n",
    "    for item in val_gp:\n",
    "        f.write(str(item))\n",
    "        if val_gp.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"val_gr: \")\n",
    "    for item in val_gr:\n",
    "        f.write(str(item))\n",
    "        if val_gr.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")  \n",
    "        \n",
    "    f.write(\"val_f1: \")\n",
    "    for item in val_f1:\n",
    "        f.write(str(item))\n",
    "        if val_f1.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")  \n",
    "    \n",
    "    f.write(\"val_kp: \")\n",
    "    for item in val_kp:\n",
    "        f.write(str(item))\n",
    "        if val_kp.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")   \n",
    "    \n",
    "    f.write(\"tes_acc: \")\n",
    "    for item in tes_acc:\n",
    "        f.write(str(item))\n",
    "        if tes_acc.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")  \n",
    "        \n",
    "    f.write(\"tes_gp: \")\n",
    "    for item in tes_gp:\n",
    "        f.write(str(item))\n",
    "        if tes_gp.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")  \n",
    "        \n",
    "    f.write(\"tes_gr: \")\n",
    "    for item in tes_gr:\n",
    "        f.write(str(item))\n",
    "        if tes_gr.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")  \n",
    "            \n",
    "    f.write(\"tes_f1: \")\n",
    "    for item in tes_f1:\n",
    "        f.write(str(item))\n",
    "        if tes_f1.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")  \n",
    "            \n",
    "    f.write(\"tes_kp: \")\n",
    "    for item in tes_kp:\n",
    "        f.write(str(item))\n",
    "        if tes_kp.index(item) == len(val_acc) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"train_time: \")\n",
    "    for item in train_time:\n",
    "        f.write(str(item))\n",
    "        if train_time.index(item) == len(train_time) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"average_results: \")\n",
    "    for item in avg_results:\n",
    "        f.write(str(item))\n",
    "        if avg_results.index(item) == len(avg_results) - 1: # Check if last item\n",
    "            f.write(';')\n",
    "        else:\n",
    "            f.write(', ')\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f19e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
