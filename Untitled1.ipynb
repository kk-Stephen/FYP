{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35325e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64da2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/ROG/OneDrive/桌面/FYP/Dataset/Train_data/train_data_after_washing.csv'\n",
    "test_dir = 'C:/Users/ROG/OneDrive/桌面/FYP/Dataset/Test_data/test_data_after_washing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2083895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_dir)\n",
    "data = pd.DataFrame(data)\n",
    "test_data = pd.read_csv(test_dir)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "data = data[[\"review\",\"rating\"]]\n",
    "test_data = test_data[[\"review\",\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad896273",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = data['rating'].map({1 : 0,\n",
    "                                     2 : 0,\n",
    "                                     3 : 0,\n",
    "                                     4 : 0,\n",
    "                                     5 : 1,\n",
    "                                     6 : 1,\n",
    "                                     7 : 1,\n",
    "                                     8 : 1,\n",
    "                                     9 : 2,\n",
    "                                     10 : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069f9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['labels'] = test_data['rating'].map({1 : 0,\n",
    "                                     2 : 0,\n",
    "                                     3 : 0,\n",
    "                                     4 : 0,\n",
    "                                     5 : 1,\n",
    "                                     6 : 1,\n",
    "                                     7 : 1,\n",
    "                                     8 : 1,\n",
    "                                     9 : 2,\n",
    "                                     10 : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70873ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"review\",\"labels\"]]\n",
    "test_data = test_data[[\"review\",\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2741a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english') #remove useless words\n",
    "    tokens = nltk.word_tokenize(text) #Tokenizers divide strings into lists of substrings\n",
    "    lower = [word.lower() for word in tokens] #remove uppercase\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    clean_text = lemm_text\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f496f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data,tfidf_vect_fit):\n",
    "    X_tfidf = tfidf_vect_fit.transform(data) #Transform doc to matrix \n",
    "    words = tfidf_vect_fit.get_feature_names() #Get features names\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "    X_tfidf_df.columns = words\n",
    "    return(X_tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92fb6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ROG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f8dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean,max_features=8000)\n",
    "tfidf_vect_fit = tfidf_vect.fit(data['review']) #learn vocabulary from train set\n",
    "X_train = vectorize(data['review'],tfidf_vect_fit)\n",
    "y_train = data[\"labels\"].to_numpy()\n",
    "X_test = vectorize(test_data['review'],tfidf_vect_fit)\n",
    "y_test = test_data[\"labels\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ce039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(model, X, y, t, d):\n",
    "    pred = model.predict(X)        \n",
    "    acc = accuracy_score(y, pred)\n",
    "    f1 = f1_score(y, pred,average='weighted')\n",
    "    prec = precision_score(y, pred,average='weighted')\n",
    "    rec = recall_score(y, pred,average='weighted')\n",
    "    kappa = cohen_kappa_score(y, pred)\n",
    "    result = {'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec, 'kappa': kappa}\n",
    "    \n",
    "    con_mat = confusion_matrix(y, pred)\n",
    "    con_mat_norm = con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis]     # 归一化\n",
    "    con_mat_norm = np.around(con_mat_norm, decimals=2)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(con_mat_norm, annot=True, cmap='Blues')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    \n",
    "    direct='C:/Users/ROG/OneDrive/桌面/FYP/Model/W2V-RF/' + t + '_' + d + '.png'\n",
    "    plt.savefig(fname=direct, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a4c7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev tools\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\dev tools\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\dev tools\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\dev tools\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "for t in [10,20,50,100,200,1000]:\n",
    "    for d in [10,50,100,300]:\n",
    "        t0 = time.time()\n",
    "        rf_classifier = RandomForestClassifier(n_estimators=t,max_depth=d)\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        t1 = time.time()\n",
    "        time_train = t1-t0\n",
    "        time_train = str(time_train) + ' s'\n",
    "        tree = str(t)\n",
    "        depth = str(d)\n",
    "        result = report_results(rf_classifier, X_test, y_test, tree, depth)\n",
    "        text_file = open('C:/Users/ROG/OneDrive/桌面/FYP/Model/W2V-RF/'+tree+'_'+depth+ '.txt', 'w')\n",
    "        text_file.write('time:'+ time_train + ';')\n",
    "        text_file.write(json.dumps(result))\n",
    "        text_file.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef5c2848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev tools\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\dev tools\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "for t in [1000,2000]:\n",
    "    for d in [10,50,100,300,500]:\n",
    "        t0 = time.time()\n",
    "        rf_classifier = RandomForestClassifier(n_estimators=t,max_depth=d)\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        t1 = time.time()\n",
    "        time_train = t1-t0\n",
    "        time_train = str(time_train) + ' s'\n",
    "        tree = str(t)\n",
    "        depth = str(d)\n",
    "        result = report_results(rf_classifier, X_test, y_test, tree, depth)\n",
    "        text_file = open('C:/Users/ROG/OneDrive/桌面/FYP/Model/W2V-RF/'+tree+'_'+depth+ '.txt', 'w')\n",
    "        text_file.write('time:'+ time_train + ';')\n",
    "        text_file.write(json.dumps(result))\n",
    "        text_file.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1bb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    x=1\n",
    "    y=2\n",
    "    z=3\n",
    "    return (x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e88fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "a,b,c = test()\n",
    "print(a,b,c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
