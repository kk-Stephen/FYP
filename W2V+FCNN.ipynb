{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/ROG/OneDrive/桌面/FYP/Dataset/Train_data/train_data_after_washing.csv'\n",
    "test_dir = 'C:/Users/ROG/OneDrive/桌面/FYP/Dataset/Test_data/test_data_after_washing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score, cohen_kappa_score, roc_curve, auc, make_scorer, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3906e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_dir)\n",
    "data = pd.DataFrame(data)\n",
    "test_data = pd.read_csv(test_dir)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "train_data = pd.DataFrame()\n",
    "val_data = pd.DataFrame()\n",
    "data = data[[\"review\",\"rating\"]]\n",
    "test_data = test_data[[\"review\",\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = data['rating'].map({1 : 0,\n",
    "                                     2 : 0,\n",
    "                                     3 : 0,\n",
    "                                     4 : 0,\n",
    "                                     5 : 1,\n",
    "                                     6 : 1,\n",
    "                                     7 : 1,\n",
    "                                     8 : 1,\n",
    "                                     9 : 2,\n",
    "                                     10 : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['labels'] = test_data['rating'].map({1 : 0,\n",
    "                                               2 : 0,\n",
    "                                               3 : 0,\n",
    "                                               4 : 0,\n",
    "                                               5 : 1,\n",
    "                                               6 : 1,\n",
    "                                               7 : 1,\n",
    "                                               8 : 1,\n",
    "                                               9 : 2,\n",
    "                                               10 : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfaeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"review\",\"labels\"]]\n",
    "y = data[\"labels\"].to_numpy()\n",
    "test_data = test_data[[\"review\",\"labels\"]]\n",
    "X = data[\"review\"].to_numpy()\n",
    "test_X = test_data[\"review\"].to_numpy()\n",
    "test_y = test_data[\"labels\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english') #remove useless words\n",
    "    tokens = nltk.word_tokenize(text) #Tokenizers divide strings into lists of substrings\n",
    "    lower = [word.lower() for word in tokens] #remove uppercase\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    clean_text = lemm_text\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a83b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english') #remove useless words\n",
    "    tokens = nltk.word_tokenize(text) #Tokenizers divide strings into lists of substrings\n",
    "    lower = [word.lower() for word in tokens] #remove uppercase\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    clean_text = lemm_text\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcbb1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    inputs = tf.keras.Input(shape=(15085,))\n",
    "    net = tf.keras.layers.Dense(128, activation='gelu', name='classifier_dense_1')(inputs)\n",
    "    net = tf.keras.layers.Dropout(0.3)(net)\n",
    "    net = tf.keras.layers.Dense(128, activation='gelu', name='classifier_dense_2')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(128, activation='gelu', name='classifier_dense_3')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(128, activation='gelu', name='classifier_dense_4')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(3, activation='softmax', name='classifier_dense_5')(net)\n",
    "    return tf.keras.Model(inputs, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53932b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873d1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007171e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257b89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b57e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec60a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338ea39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df7b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81a459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f69c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c58b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54db600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
